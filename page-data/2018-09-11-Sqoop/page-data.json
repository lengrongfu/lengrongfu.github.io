{
    "componentChunkName": "component---src-templates-blog-post-js",
    "path": "/2018-09-11-Sqoop/",
    "result": {"data":{"site":{"siteMetadata":{"title":"LRF"}},"markdownRemark":{"id":"190ee72d-5e39-5eb8-bac7-160d321a64ef","excerpt":"1、简介 是一种用于转换和关系型数据库之间的数据。你可以导出关系型数据库如到上，在上进行数据转换，然后在导出到关系型数据库中。依靠数据库来描述要导入的数据的模式，使用来做数据的导入和导出，它提供并行操作和容错. sqoop…","html":"<h1>1、简介</h1>\n<blockquote>\n<p><code class=\"language-text\">Sqoop</code>是一种用于转换<code class=\"language-text\">Hadoop</code>和关系型数据库之间的数据。你可以导出关系型数据库如<code class=\"language-text\">mysql</code>到<code class=\"language-text\">HDFS</code>上，在<code class=\"language-text\">Hadoop MapReduce</code>上进行数据转换，然后在导出到关系型数据库中。<code class=\"language-text\">Sqoop</code>依靠数据库来描述要导入的数据的模式，<code class=\"language-text\">Sqoop</code>使用<code class=\"language-text\">MapReduce</code>来做数据的导入和导出，它提供并行操作和容错.</p>\n</blockquote>\n<p><img src=\"sqoop.png\" alt=\"sqoop\"></p>\n<h1>2、基本使用</h1>\n<blockquote>\n<p>使用<code class=\"language-text\">Sqoop</code>，可以把关系型数据库中的数据导入到<code class=\"language-text\">HDFS</code>中。对于数据库，<code class=\"language-text\">Sqoop</code>将逐行读取表格到<code class=\"language-text\">HDFS</code>,此导入过程的输出是一组文件，其中包含导入的表或数据集的副本(<code class=\"language-text\">parquet</code>)，导入是由<code class=\"language-text\">MapReduce</code>并行执行的，因此输出会在多个<code class=\"language-text\">parquet</code>文件中。</p>\n</blockquote>\n<blockquote>\n<p>导入过程的副产品是生成的java类，</p>\n</blockquote>\n<blockquote>\n<p>在进行数据操作之后(使用<code class=\"language-text\">MapReduce</code>或者<code class=\"language-text\">Hive</code>)计算之后，会有一个结果集，然后你可以将其导出回关系型数据库中，<code class=\"language-text\">Sqoop</code>的导出过程将并行读取<code class=\"language-text\">HDFS</code>中的一组分割文本文件，将他们解析为记录，并将它们作为新行插入目录数据库表中，供外部应用程序或用户使用。</p>\n</blockquote>\n<blockquote>\n<p><code class=\"language-text\">Sqoop</code>包含一些其它命令，如：可以列出目前所有可用的数据库，<code class=\"language-text\">sqoop-list-databases --connect=jdbc:mysql://10.1.4.99:3306/test --username=lrf --password=lrf</code>,列出数据库中的所有表：<code class=\"language-text\">sqoop-list-tables --connect=jdbc:mysql://10.1.4.99:3306/test --username=lrf --password=lrf</code>,还包括一个原始的SQl执行shell:<code class=\"language-text\">sqoop-eval</code></p>\n</blockquote>\n<blockquote>\n<p>可以自定义导入,对于数据库，可以控制导入的特定行范围或列，</p>\n</blockquote>\n<h1>3、Sqoop 工具</h1>\n<p><code class=\"language-text\">Sqoop</code>时相关工具的集合，要使用<code class=\"language-text\">Sqoop</code>,需要指定要使用的工具和控制工具的参数。</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$ sqoop -help\nusage: sqoop COMMAND <span class=\"token punctuation\">[</span>ARGS<span class=\"token punctuation\">]</span>\n\nAvailable commands:\n  codegen            Generate code to interact with database records <span class=\"token comment\">#生成与数据库交互的代码</span>\n  create-hive-table  Import a table definition into Hive <span class=\"token comment\"># 导入表定义到hive</span>\n  <span class=\"token builtin class-name\">eval</span>               Evaluate a SQL statement and display the results <span class=\"token comment\"># 评估sql语句并显示结果</span>\n  <span class=\"token builtin class-name\">export</span>             Export an HDFS directory to a database table <span class=\"token comment\"># 将HDFS目录导出到数据库</span>\n  <span class=\"token builtin class-name\">help</span>               List available commands <span class=\"token comment\"># 列出可用命令</span>\n  <span class=\"token function\">import</span>             Import a table from a database to HDFS <span class=\"token comment\"># 从数据库中导入一个指定表到HDFS</span>\n  import-all-tables  Import tables from a database to HDFS <span class=\"token comment\"># 把一个数据库中的表都导入到HDFS</span>\n  import-mainframe   Import mainframe datasets to HDFS <span class=\"token comment\"># 导入主数据库到HDFS</span>\n  list-databases     List available databases on a server <span class=\"token comment\"># </span>\n  list-tables        List available tables <span class=\"token keyword\">in</span> a database\n  version            Display version information\n\nSee <span class=\"token string\">'sqoop help COMMAND'</span> <span class=\"token keyword\">for</span> information on a specific command.</code></pre></div>\n<h1>4、从mysql 到 HDFS</h1>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token comment\"># 导入指定的表</span>\n$ sqoop <span class=\"token function\">import</span> --connect jdbc:mysql://<span class=\"token operator\">&lt;</span>dburi<span class=\"token operator\">></span>/<span class=\"token operator\">&lt;</span>dbname<span class=\"token operator\">></span>?zeroDateTimeBehavior<span class=\"token operator\">=</span>CONVERT_TO_NULL <span class=\"token comment\"># db</span>\n                --username <span class=\"token operator\">&lt;</span>username<span class=\"token operator\">></span> <span class=\"token comment\"># db user</span>\n                --password <span class=\"token operator\">&lt;</span>password<span class=\"token operator\">></span> <span class=\"token comment\"># db pwd</span>\n                --table <span class=\"token operator\">&lt;</span>tablename<span class=\"token operator\">></span> <span class=\"token comment\"># db table</span>\n                --check-column <span class=\"token operator\">&lt;</span>col<span class=\"token operator\">></span> <span class=\"token comment\"># 要检查的列的名称</span>\n                --incremental <span class=\"token operator\">&lt;</span>mode<span class=\"token operator\">></span>  <span class=\"token comment\"># 改模式决定sqoop如何定义哪些行为新的行。取值为append或lastmodified</span>\n                --last-value <span class=\"token operator\">&lt;</span>value<span class=\"token operator\">></span> <span class=\"token comment\"># 前一个导入中检查列的最大值</span>\n                --target-dir <span class=\"token operator\">&lt;</span>hdfs-dir<span class=\"token operator\">></span> <span class=\"token comment\"># HDFS 的写入目录，例如：/user/hdfs/result.</span>\n<span class=\"token comment\"># 列子</span>\nsqoop <span class=\"token function\">import</span> --connect jdbc:mysql://10.1.4.99:3306/test?zeroDateTimeBehavior<span class=\"token operator\">=</span>CONVERT_TO_NULL --table student --username lrf --password lrf --target-dir hdfs://hadoop-11.146.dev.net/user/hdfs/test/student\n\n<span class=\"token comment\"># 导入数据库下的所有表</span>\nsqoop import-all-tables <span class=\"token punctuation\">\\</span>\n    --username <span class=\"token operator\">&lt;</span>username<span class=\"token operator\">></span> <span class=\"token comment\"># db user</span>\n    --password <span class=\"token operator\">&lt;</span>password<span class=\"token operator\">></span> <span class=\"token comment\"># db pwd</span>\n    --check-column <span class=\"token operator\">&lt;</span>col<span class=\"token operator\">></span> <span class=\"token comment\"># 要检查的列的名称</span>\n    --incremental <span class=\"token operator\">&lt;</span>mode<span class=\"token operator\">></span>  <span class=\"token comment\"># 改模式决定sqoop如何定义哪些行为新的行。取值为append或lastmodified</span>\n    --last-value <span class=\"token operator\">&lt;</span>value<span class=\"token operator\">></span> <span class=\"token comment\"># 前一个导入中检查列的最大值</span>\n<span class=\"token comment\"># 列子,不能指定导入，默认会全部导入到/user/hdfs目录下</span>\nsqoop import-all-tables --connect jdbc:mysql://10.1.4.99:3306/test?zeroDateTimeBehavior<span class=\"token operator\">=</span>CONVERT_TO_NULL --username lrf --password lrf </code></pre></div>\n<h1>5、从HDFS 到Mysql</h1>\n<blockquote>\n<p>需要先创建好对应 HDFS 中的数据结构的 MySQL 表，然后在集群的 Master 节点上执行如下命令，指定要导的数据文件的路径</p>\n</blockquote>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$ sqoop <span class=\"token builtin class-name\">export</span> --connect jdbc:mysql://<span class=\"token operator\">&lt;</span>dburi<span class=\"token operator\">></span>/<span class=\"token operator\">&lt;</span>dbname<span class=\"token operator\">></span> \n               --username <span class=\"token operator\">&lt;</span>username<span class=\"token operator\">></span> \n               --password <span class=\"token operator\">&lt;</span>password<span class=\"token operator\">></span> \n               --table <span class=\"token operator\">&lt;</span>tablename<span class=\"token operator\">></span> \n               --export-dir <span class=\"token operator\">&lt;</span>hdfs-dir<span class=\"token operator\">></span>\n<span class=\"token comment\"># 列子</span>\nsqoop <span class=\"token builtin class-name\">export</span> --connect jdbc:mysql://10.1.4.99:3306/test --username lrf --password lrf --table student \n--export-dir hdfs://hadoop-11.146.dev.net/user/hdfs/test/student</code></pre></div>\n<h1>6、从mysql 到Hive</h1>\n<blockquote>\n<p>在集群的 Master 节点上执行如下命令后，从MySQL数据库导入数据的同时，也会新建一个 Hive 表。</p>\n</blockquote>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$ sqoop <span class=\"token function\">import</span> --connect jdbc:mysql://<span class=\"token operator\">&lt;</span>dburi<span class=\"token operator\">></span>/<span class=\"token operator\">&lt;</span>dbname<span class=\"token operator\">></span> \n               --username <span class=\"token operator\">&lt;</span>username<span class=\"token operator\">></span> \n               --password <span class=\"token operator\">&lt;</span>password<span class=\"token operator\">></span> \n               --table <span class=\"token operator\">&lt;</span>tablename<span class=\"token operator\">></span> \n               --check-column <span class=\"token operator\">&lt;</span>col<span class=\"token operator\">></span> \n               --incremental <span class=\"token operator\">&lt;</span>mode<span class=\"token operator\">></span> \n               --last-value <span class=\"token operator\">&lt;</span>value<span class=\"token operator\">></span> \n               --fields-terminated-by <span class=\"token string\">\"<span class=\"token entity\" title=\"\\t\">\\t</span>\"</span> \n               --lines-terminated-by <span class=\"token string\">\"<span class=\"token entity\" title=\"\\n\">\\n</span>\"</span> \n               --hive-import \n               --warehouse-dir <span class=\"token operator\">&lt;</span>hdfs-dir<span class=\"token operator\">></span> \n               --hive-table <span class=\"token operator\">&lt;</span>hive-tablename<span class=\"token operator\">></span>\n<span class=\"token comment\"># 列子</span>\nsqoop import-all-tables <span class=\"token punctuation\">\\</span>\n    -m <span class=\"token number\">3</span> <span class=\"token punctuation\">\\</span> \n    --connect jdbc:mysql://10.1.4.99:3306/test?zeroDateTimeBehavior<span class=\"token operator\">=</span>CONVERT_TO_NULL <span class=\"token punctuation\">\\</span> <span class=\"token comment\">#mysql连接，zeroDateTimeBehavior 处理异常时间的配置,比如0，</span>\n    --username<span class=\"token operator\">=</span>lrf <span class=\"token punctuation\">\\</span>\n    --password<span class=\"token operator\">=</span>lrf <span class=\"token punctuation\">\\</span>\n    --compression-codec<span class=\"token operator\">=</span>snappy <span class=\"token punctuation\">\\</span>\n    --as-parquetfile <span class=\"token punctuation\">\\</span>\n    --warehouse-dir<span class=\"token operator\">=</span>/user/hive/warehouse <span class=\"token punctuation\">\\</span>\n    --hive-import</code></pre></div>\n<h1>7、从Hive到mysql</h1>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">$ sqoop <span class=\"token builtin class-name\">export</span> --connect jdbc:mysql://<span class=\"token operator\">&lt;</span>dburi<span class=\"token operator\">></span>/<span class=\"token operator\">&lt;</span>dbname<span class=\"token operator\">></span> \n               --username <span class=\"token operator\">&lt;</span>username<span class=\"token operator\">></span> \n               --password <span class=\"token operator\">&lt;</span>password<span class=\"token operator\">></span> \n               --table <span class=\"token operator\">&lt;</span>tablename<span class=\"token operator\">></span> \n               --export-dir <span class=\"token operator\">&lt;</span>hive-dir<span class=\"token operator\">></span>\n\n<span class=\"token comment\"># 例子,如果导出时要指定hive表的字段，不然分割不了，导出默认是使用逗号分割的，需要指定hive的列对应上mysql中的列，下列中的columns字段就是hive表中的字段。</span>\nsqoop <span class=\"token builtin class-name\">export</span> --connect jdbc:mysql://10.1.4.99:3306/test --username lrf --password lrf --table nginx_access_logs --columns reote_addr,time_local,request,status,body_bytes_sent,http_referer,http_user_agent,http_x_forwarded_for --export-dir hdfs://hadoop-11.146.dev.net/user/hive/warehouse/nginx_access_logs</code></pre></div>\n<h1>8、使用sql条件导入到hdfs或hive</h1>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">sqoop <span class=\"token function\">import</span> --connect jdbc:mysql://<span class=\"token operator\">&lt;</span>dburi<span class=\"token operator\">></span>/<span class=\"token operator\">&lt;</span>dbname<span class=\"token operator\">></span> \n             --username <span class=\"token operator\">&lt;</span>username<span class=\"token operator\">></span> \n             --password <span class=\"token operator\">&lt;</span>password<span class=\"token operator\">></span> \n             --query <span class=\"token operator\">&lt;</span>query-sql<span class=\"token operator\">></span> \n             --split-by <span class=\"token operator\">&lt;</span>sp-column<span class=\"token operator\">></span> <span class=\"token comment\"># 查询的条件,如:'select * from student where age > 20'</span>\n             --hive-import \n             --hive-table <span class=\"token operator\">&lt;</span>hive-tablename<span class=\"token operator\">></span> \n             --target-dir <span class=\"token operator\">&lt;</span>hdfs-dir<span class=\"token operator\">></span></code></pre></div>\n<h1>参考</h1>\n<ul>\n<li><a href=\"https://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html\">https://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html</a></li>\n<li><a href=\"https://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html#_connecting_to_a_database_server\">https://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html#_connecting_to_a_database_server</a></li>\n</ul>","frontmatter":{"title":"Sqoop User Guide(Sqoop使用指南)","date":"September 11, 2018","description":null}},"previous":{"fields":{"slug":"/2018-09-06-cloudera/"},"frontmatter":{"title":"cloudera hadoop 安装"}},"next":{"fields":{"slug":"/2018-10-16-cloudera-problem/"},"frontmatter":{"title":"cloudera 问题解决"}}},"pageContext":{"id":"190ee72d-5e39-5eb8-bac7-160d321a64ef","previousPostId":"e48b00ed-64f8-57e6-ab8a-690931bab7d7","nextPostId":"cd3b5f76-78e9-5d38-bca8-83d1126a6d8f"}},
    "staticQueryHashes": ["2841359383","3257411868"]}